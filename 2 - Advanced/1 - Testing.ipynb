{
 "metadata": {
  "name": "",
  "signature": "sha256:689118b31e5814c28f8688188ca691b2bc889ae64540c755e5b82d9e7be68fe1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Testing your code\n",
      "=================\n",
      "\n",
      "Testing your code is the most important tasks you will have to do.\n",
      "\n",
      "This task is simplified by using small, well defined function, which can be tested easily.\n",
      "\n",
      "They are different way/flavor of doing tests with Python, here are the main ones. The one that you could/should pick will mainly be determined by the size/complexity of project and the continuous integration tools that it may use."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets define a simple function which only return the input plus one :"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file func.py\n",
      "def func(x):\n",
      "    return x + 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Writing func.py\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from func import func\n",
      "assert func(3) == 4 # assert can check that the function is working properly\n",
      "func(3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "4"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assert func(3.) == 4.0 # the function also works with float\n",
      "func(3.)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "4.0"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "doctest\n",
      "-------\n",
      "\n",
      "[https://docs.python.org/library/doctest.html]\n",
      "\n",
      "Doctest is a simple way of testing the output of your function. One just add some examples to the docstring of the function. It is basically the calling sequence and the expected result as if run from the python prompt.\n",
      "\n",
      "A doctest launch sequence is also added when the file is run directly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file func.py \n",
      "def func(x):\n",
      "    \"\"\"\n",
      "    This function add one to the input.\n",
      "    \n",
      "    >>> func(3)\n",
      "    4\n",
      "    >>> func(3.)\n",
      "    4.0\n",
      "    \"\"\"\n",
      "    return x+1\n",
      "\n",
      "# Let's add this, so that if we run the file, python will launch the doctests\n",
      "if __name__ in (\"__main__\", \"__console__\"):\n",
      "    import doctest\n",
      "    doctest.testmod(verbose=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting func.py\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run func"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Trying:\n",
        "    func(3)\n",
        "Expecting:\n",
        "    4\n",
        "ok\n",
        "Trying:\n",
        "    func(3.)\n",
        "Expecting:\n",
        "    4.0\n",
        "ok\n",
        "1 items had no tests:\n",
        "    __main__\n",
        "1 items passed all tests:\n",
        "   2 tests in __main__.func\n",
        "2 tests in 2 items.\n",
        "2 passed and 0 failed.\n",
        "Test passed.\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "inittest\n",
      "--------\n",
      "\n",
      "[https://docs.python.org/2/library/unittest.html]\n",
      "\n",
      "As doctest, inittest is part of the python distribution. Here we will define a new class derived from unittest.TestCase to test our function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file test_func_unittest.py\n",
      "from func import func\n",
      "import unittest\n",
      "\n",
      "class TestFunc(unittest.TestCase):\n",
      "\n",
      "\n",
      "    def test_int(self):\n",
      "        self.assertEqual(func(3), 4)\n",
      "\n",
      "    def test_float(self):\n",
      "        self.assertEqual(func(3.), 4)\n",
      "\n",
      "    # To return a failure...\n",
      "    def test_bad(self):\n",
      "        self.assertEqual(func(3), 5)\n",
      "        \n",
      "if __name__ == '__main__':\n",
      "    unittest.main()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting test_func_unittest.py\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run test_func_unittest"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "F"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "======================================================================\n",
        "FAIL: test_bad (__main__.TestFunc)\n",
        "----------------------------------------------------------------------\n",
        "Traceback (most recent call last):\n",
        "  File \"/home/abeelen/Documents/Python Notebook/2 - Advanced/test_func_unittest.py\", line 16, in test_bad\n",
        "    self.assertEqual(func(3), 5)\n",
        "AssertionError: 4 != 5\n",
        "\n",
        "----------------------------------------------------------------------\n",
        "Ran 3 tests in 0.002s\n",
        "\n",
        "FAILED (failures=1)\n"
       ]
      },
      {
       "ename": "SystemExit",
       "evalue": "True",
       "output_type": "pyerr",
       "traceback": [
        "An exception has occurred, use %tb to see the full traceback.\n",
        "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m True\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "PyTest\n",
      "------\n",
      "\n",
      "Pytest is based on a simpler syntax. We just to write a serie of small functions to test all possible use case of func(x).\n",
      "\n",
      "Let's put all this in a file, by convention just prefixed with `test_`. Here we also add a postfix to separate inittest and pytest, however this will disable pytest automatic test search capabilities."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file test_func_pytest.py\n",
      "from func import func\n",
      "\n",
      "def test_int():\n",
      "    assert func(3) == 4\n",
      "\n",
      "# Here we test that the function also work with float AND return the proper type\n",
      "def test_float():\n",
      "    value = func(3.)\n",
      "    assert value == 4.\n",
      "    assert type(value) == float\n",
      "    \n",
      "# To return a failure...\n",
      "def test_bad():\n",
      "    assert func(3) == 5 "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting test_func_pytest.py\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use pytest to automaticaly test for our function, and report the failed tests"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "! py.test test_func_pytest.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
        "platform linux2 -- Python 2.7.8 -- py-1.4.25 -- pytest-2.6.3\r\n",
        "\u001b[1m\r",
        "collecting 0 items\u001b[0m\u001b[1m\r",
        "collecting 3 items\u001b[0m\u001b[1m\r",
        "collected 3 items \r\n",
        "\u001b[0m\r\n",
        "test_func_pytest.py .FF\r\n",
        "\r\n",
        "=================================== FAILURES ===================================\r\n",
        "__________________________________ test_float __________________________________\r\n",
        "\r\n",
        "\u001b[1m    def test_float():\u001b[0m\r\n",
        "\u001b[1m        value = func(3.)\u001b[0m\r\n",
        "\u001b[1m        assert value == 4.\u001b[0m\r\n",
        "\u001b[1m>       assert type(value) == float\u001b[0m\r\n",
        "\u001b[1m\u001b[31mE       assert <type 'int'> == float\u001b[0m\r\n",
        "\u001b[1m\u001b[31mE        +  where <type 'int'> = type(4)\u001b[0m\r\n",
        "\r\n",
        "test_func_pytest.py:10: AssertionError\r\n",
        "___________________________________ test_bad ___________________________________\r\n",
        "\r\n",
        "\u001b[1m    def test_bad():\u001b[0m\r\n",
        "\u001b[1m>       assert func(3) == 5\u001b[0m\r\n",
        "\u001b[1m\u001b[31mE       assert 4 == 5\u001b[0m\r\n",
        "\u001b[1m\u001b[31mE        +  where 4 = func(3)\u001b[0m\r\n",
        "\r\n",
        "test_func_pytest.py:14: AssertionError\r\n",
        "\u001b[1m\u001b[31m====================== 2 failed, 1 passed in 0.01 seconds ======================\u001b[0m\r\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Regression \n",
      "==========\n",
      "\n",
      "Let's imagine that the test_func cover all planned usage of func().\n",
      "\n",
      "Now, for some reason, we decide to change func, to return only an integer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file func.py\n",
      "\n",
      "def func(x):\n",
      "    \"\"\"\n",
      "    This function add one to the input.\n",
      "    \n",
      "    >>> func(3)\n",
      "    4\n",
      "    >>> func(3.)\n",
      "    4.0\n",
      "    \"\"\"\n",
      "\n",
      "    return int(x + 1)\n",
      "\n",
      "\n",
      "# Let's add this, so that if we run the file, python will launch the doctests\n",
      "if __name__ in (\"__main__\", \"__console__\"):\n",
      "    import doctest\n",
      "    doctest.testmod(verbose=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting func.py\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a clear regression of the code, which could impact functions elsewhere, and here all tests will pick it up"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run func"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Trying:\n",
        "    func(3)\n",
        "Expecting:\n",
        "    4\n",
        "ok\n",
        "Trying:\n",
        "    func(3.)\n",
        "Expecting:\n",
        "    4.0\n",
        "**********************************************************************\n",
        "File \"/home/abeelen/Documents/Python Notebook/2 - Advanced/func.py\", line 8, in __main__.func\n",
        "Failed example:\n",
        "    func(3.)\n",
        "Expected:\n",
        "    4.0\n",
        "Got:\n",
        "    4\n",
        "1 items had no tests:\n",
        "    __main__\n",
        "**********************************************************************\n",
        "1 items had failures:\n",
        "   1 of   2 in __main__.func\n",
        "2 tests in 2 items.\n",
        "1 passed and 1 failed.\n",
        "***Test Failed*** 1 failures.\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run test_func_unittest"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "F"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "======================================================================\n",
        "FAIL: test_bad (__main__.TestFunc)\n",
        "----------------------------------------------------------------------\n",
        "Traceback (most recent call last):\n",
        "  File \"/home/abeelen/Documents/Python Notebook/2 - Advanced/test_func_unittest.py\", line 16, in test_bad\n",
        "    self.assertEqual(func(3), 5)\n",
        "AssertionError: 4 != 5\n",
        "\n",
        "----------------------------------------------------------------------\n",
        "Ran 3 tests in 0.001s\n",
        "\n",
        "FAILED (failures=1)\n"
       ]
      },
      {
       "ename": "SystemExit",
       "evalue": "True",
       "output_type": "pyerr",
       "traceback": [
        "An exception has occurred, use %tb to see the full traceback.\n",
        "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m True\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "PyTest can also run test from unittest files, so one can directly run it in the directory"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "! py.test "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
        "platform linux2 -- Python 2.7.8 -- py-1.4.25 -- pytest-2.6.3\r\n",
        "\u001b[1m\r",
        "collecting 0 items\u001b[0m\u001b[1m\r",
        "collecting 3 items\u001b[0m\u001b[1m\r",
        "collecting 6 items\u001b[0m\u001b[1m\r",
        "collecting 6 items\u001b[0m\u001b[1m\r",
        "collected 6 items \r\n",
        "\u001b[0m\r\n",
        "test_func_pytest.py .FF"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n",
        "test_func_unittest.py F..\r\n",
        "\r\n",
        "=================================== FAILURES ===================================\r\n",
        "__________________________________ test_float __________________________________\r\n",
        "\r\n",
        "\u001b[1m    def test_float():\u001b[0m\r\n",
        "\u001b[1m        value = func(3.)\u001b[0m\r\n",
        "\u001b[1m        assert value == 4.\u001b[0m\r\n",
        "\u001b[1m>       assert type(value) == float\u001b[0m\r\n",
        "\u001b[1m\u001b[31mE       assert <type 'int'> == float\u001b[0m\r\n",
        "\u001b[1m\u001b[31mE        +  where <type 'int'> = type(4)\u001b[0m\r\n",
        "\r\n",
        "test_func_pytest.py:10: AssertionError\r\n",
        "___________________________________ test_bad ___________________________________\r\n",
        "\r\n",
        "\u001b[1m    def test_bad():\u001b[0m\r\n",
        "\u001b[1m>       assert func(3) == 5\u001b[0m\r\n",
        "\u001b[1m\u001b[31mE       assert 4 == 5\u001b[0m\r\n",
        "\u001b[1m\u001b[31mE        +  where 4 = func(3)\u001b[0m\r\n",
        "\r\n",
        "test_func_pytest.py:14: AssertionError\r\n",
        "______________________________ TestFunc.test_bad _______________________________\r\n",
        "\r\n",
        "self = <test_func_unittest.TestFunc testMethod=test_bad>\r\n",
        "\r\n",
        "\u001b[1m    def test_bad(self):\u001b[0m\r\n",
        "\u001b[1m>       self.assertEqual(func(3), 5)\u001b[0m\r\n",
        "\u001b[1m\u001b[31mE       AssertionError: 4 != 5\u001b[0m\r\n",
        "\r\n",
        "test_func_unittest.py:16: AssertionError\r\n",
        "\u001b[1m\u001b[31m====================== 3 failed, 3 passed in 0.04 seconds ======================\u001b[0m\r\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}